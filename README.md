# Memory Chatbot 3004 â€“ **HealthGPT Edition** ğŸ©ºğŸ¤–

> **Disclaimer**â€‚This demo is *not* a certified medical device. It offers informational support only and always reminds users to consult qualified healthcare professionals.

---

## ğŸ–¼ï¸ Reference Architecture

> *![](images/Long-term-memory.png)*

---

## ğŸš€ Overview

`HealthGPT` is a Streamlitâ€‘powered chatbot that **remembers each userâ€™s history** via vector embeddings (Pinecone) and dishes out healthcareâ€‘flavoured answers with a hint of wit.  Longâ€‘term memories live in Mem0; a YAML file (`prompts.yaml`) holds the system prompt so nonâ€‘coders can tweak the botâ€™s behaviour without touching Python.

| Stack                   | Why                                                 |
| ----------------------- | --------------------------------------------------- |
| **Streamlit**           | Quick chat UI with zero frontâ€‘end tears             |
| **OpenAI GPTâ€‘4oâ€‘mini**  | Core reasoning LLM                                  |
| **Mem0**                | Simple wrapper for longâ€‘term memory CRUD            |
| **Pinecone Serverless** | Vector storage (1536â€‘dim embeddings)                |
| **YAML**                | Externalised system prompt (nonâ€‘devâ€‘friendly edits) |

---

## âœ¨ Features

* **Userâ€‘specific longâ€‘term memory** â€“ retrieves topâ€‘K nuggets from Mem0 and writes the *last 5 chat bubbles* back after every turn (batched, nonâ€‘blocking).
* **Continuous context** â€“ sends up to the *20 most recent messages* to GPTâ€‘4oâ€‘mini so the conversation never loses its thread.
* **Async OpenAI calls** â€“ leverages `AsyncOpenAI` + a threadâ€‘pool for faster, nonâ€‘blocking responses in Streamlit.
* **Healthcare guardrails** â€“ goal & safety instructions live in `prompts.yaml`; tweak them without touching Python.
* **Hotâ€‘reloadable prompts** â€“ edit `prompts.yaml`, save, refresh the browserâ€”changes apply instantly via `st.cache_data`.
* **Streamlit niceties** â€“ darkâ€‘modeâ€‘friendly chat bubbles, sidebar controls, username switch, and memoryâ€‘expander for debugging.
* **Graceful memory batching** â€“ messages queue until 5 are ready, then persist to Pinecone in one shot to reduce API chatter.


## ğŸ“‚ Project structure

```
â”œâ”€â”€ main.py            # Streamlit app (was app.py)
â”œâ”€â”€ Images             # Images Folder
    â”œâ”€â”€Long-term-memory.png         
â”œâ”€â”€ prompts.yaml       # System prompt & backgroundâ€‘task spec
â”œâ”€â”€ requirements.txt   # Python deps
â””â”€â”€ README.md          # Youâ€™re here

```

---

## ğŸ› ï¸ Prerequisites

* Python **3.10+**
* Valid **OpenAI API key**
* Valid **Pinecone API key** (serverless)
* Suggested: virtualenv (venv, pipenv, poetry)

---

## ğŸ”§ Installation

```bash
# 1. Clone & step inside
git clone <github repo link>
cd healthgpt-chatbot


# 2. Create & activate virtual env
python -m venv .venv

MACOS: source .venv/bin/activate  
Windows: .venv\Scripts\activate

# 3. Install deps
pip install uv 
uv pip install -r requirements.txt #Neat trick for blazing fast package management

# 4. Create .env file
cp .env-example .env # Macos
copy .env-example .env # Windows
```
---

## ğŸ” Configuration

Edit the **.env** in the project root:

```ini
OPENAI_API_KEY=sk-â€¢â€¢â€¢
PINECONE_API_KEY=â€¢â€¢â€¢
```

Tweaks:

* **Collection name** / embedding dims â€“ see `main.py > memory_store` config.
* **System prompt** â€“ edit `prompts.yaml`; save & refresh.

---

## ğŸƒâ€â™‚ï¸ Running locally

```bash
streamlit run main.py
```

Visit [http://localhost:8501](http://localhost:8501) and pick a username.
